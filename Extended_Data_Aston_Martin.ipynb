{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f204158-292e-4d31-beeb-de1b2ad121e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Aston_Martin\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Current year for the 'Brand New' cars\n",
    "current_year = datetime.now().year\n",
    "\n",
    "# Load the data\n",
    "file_path = 'cleaned_auto_trader_data_aston_martin.csv' # Has to be the cleaned data as this is file that's updated\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Function to extract numeric part from a string (for 'Price' and 'Mileage')\n",
    "def extract_numeric(value):\n",
    "    try:\n",
    "        # Remove non-digit characters, preserve decimal points and negative signs\n",
    "        numeric_string = re.sub(\"[^-0-9.]\", \"\", str(value))\n",
    "        return float(numeric_string)\n",
    "    except ValueError:\n",
    "        return np.nan  # Return NaN for non-numeric or empty values\n",
    "\n",
    "# Clean the 'Year' column by removing parentheses and their contents\n",
    "data['Year'] = data['Year'].astype(str).apply(lambda x: re.sub(r'\\s*\\(.*\\)', '', x)).astype(float)\n",
    "\n",
    "# Apply transformations and validations\n",
    "data['Price'] = data['Price'].apply(extract_numeric)\n",
    "data['Mileage'] = data['Mileage'].apply(extract_numeric)\n",
    "data['Prior_Owners'] = data['Prior_Owners'].apply(extract_numeric)  # Updated to use extract_numeric\n",
    "\n",
    "# Fill in missing data with 'Unknown' only for Engine_Size column\n",
    "data['Engine_Size'] = data['Engine_Size'].fillna('Unknown')\n",
    "\n",
    "# Fill in missing data with 'Unknown' for other categorical columns\n",
    "categorical_columns = ['Model', 'Additional_Comments', 'Car_Type', 'Engine_Type', 'Dealership_Location', 'Sale_Type']\n",
    "data[categorical_columns] = data[categorical_columns].fillna('Unknown')\n",
    "\n",
    "# Remove rows with 'Unknown' values in critical columns\n",
    "data = data[data['Price'] != 'Unknown']\n",
    "data = data[data['Mileage'] != 'Unknown']\n",
    "data = data[data['Year'] != 'Unknown']\n",
    "\n",
    "# Adding the delete duplicates feature\n",
    "cleaned_data = data.drop_duplicates()\n",
    "\n",
    "# Save the cleaned data to a new file\n",
    "cleaned_file_path = 'cleaned_auto_trader_data_aston_martin.csv'\n",
    "data.to_csv(cleaned_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d390597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPLORATORY DATA ANALYSIS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f4bb79f-4cf5-407d-9d6d-2b4231f3c925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Preprocessing numeric columns...\n",
      "Creating preprocessing pipelines...\n",
      "Applying TF-IDF Vectorization...\n",
      "Splitting data...\n",
      "Training model with hyperparameter tuning...\n",
      "Best model found: RandomForestRegressor(max_depth=20, min_samples_split=10, n_estimators=50,\n",
      "                      random_state=42)\n",
      "Making predictions and evaluating...\n",
      "MSE: 1250240567.3750648\n",
      "R2 Score: 0.8530516286395321\n",
      "Saving model, vectorizer, and preprocessor...\n",
      "Process completed.\n"
     ]
    }
   ],
   "source": [
    "# For Aston_Martin - Extended Data\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Load the data\n",
    "print(\"Loading data...\")\n",
    "file_path = 'cleaned_auto_trader_data_aston_martin.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Preprocess numeric columns\n",
    "print(\"Preprocessing numeric columns...\")\n",
    "for col in ['Price', 'Mileage', 'Year', 'Prior_Owners']:\n",
    "    data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "    data[col].fillna(data[col].mean(), inplace=True)\n",
    "\n",
    "# Preprocessing pipelines\n",
    "print(\"Creating preprocessing pipelines...\") # TODO visualise the data after and before preprocessing to see if any issues with it\n",
    "numeric_features = ['Mileage', 'Year', 'Prior_Owners']\n",
    "numeric_transformer = StandardScaler()\n",
    "\n",
    "categorical_features = ['Make', 'Car_Type', 'Sale_Type']\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# TF-IDF Vectorizer for 'Model' column\n",
    "print(\"Applying TF-IDF Vectorization...\") # TODO check results of this vectorisation. not sure TF-IDF is needed for \"Model\" of car tbh but interesting.\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=2500)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(data['Model'])\n",
    "\n",
    "# Fit and transform the data using the preprocessor\n",
    "X_preprocessed = preprocessor.fit_transform(data.drop('Model', axis=1)) # TODO check the data here is not a concatination, but just the preprocessed data \n",
    "\n",
    "# Combine TF-IDF features with preprocessed numeric and categorical features # TODO investigate how this data looks, both TFIDF and preprocessed, to see if we really can expect model to fit to both at same time\n",
    "X_combined = np.hstack((X_tfidf.toarray(), X_preprocessed if isinstance(X_preprocessed, np.ndarray) else X_preprocessed.toarray()))\n",
    "# TODO I think bunching this data together and using random forest isn't a good combination. Need to visualise both the TFIDF data and preprocessed data, seperately, and both before and after preprocessing. Do some exploratory data analysis. Visualise words with word map (bigger words for more frequent) and visualise numeric data with grid map different variables on each axis of grid\n",
    "\n",
    "# Split the data\n",
    "print(\"Splitting data...\")\n",
    "y = data['Price'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# TODO try and get random forest to show which features it is weighting (importances)\n",
    "\n",
    "# Model Training with Hyperparameter Tuning\n",
    "print(\"Training model with hyperparameter tuning...\")\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Use the best estimator # TODO investigate the \"performance\" of each model with different hyperparamter tuning. Check it's actually finding a good one, not just the least poor performing.\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"Best model found:\", best_model)\n",
    "\n",
    "# Predictions and Evaluation\n",
    "print(\"Making predictions and evaluating...\")\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Output the model performance\n",
    "print(f'MSE: {mse}')\n",
    "print(f'R2 Score: {r2}') # TODO this score seems good. but not great. Investigate examples where it's accurate, and where it's messing up. see if there's anything obvious. back to EDA\n",
    "\n",
    "# Save the model, vectorizer, and preprocessor\n",
    "print(\"Saving model, vectorizer, and preprocessor...\")\n",
    "joblib.dump(best_model, 'car_price_prediction_model_aston_martin.joblib')\n",
    "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer_aston_martin.joblib')\n",
    "joblib.dump(preprocessor, 'preprocessor_aston_martin.joblib')\n",
    "\n",
    "print(\"Process completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7810a5f-8784-4fb0-b513-afd1954330b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Car: Aston Martin Vanquish, Predicted Price: Â£63102.17\n"
     ]
    }
   ],
   "source": [
    "# Aston_Martin - Extended Data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Load the saved model, vectorizer, and preprocessor\n",
    "model = joblib.load('car_price_prediction_model_aston_martin.joblib')\n",
    "vectorizer = joblib.load('tfidf_vectorizer_aston_martin.joblib')\n",
    "preprocessor = joblib.load('preprocessor_aston_martin.joblib')\n",
    "\n",
    "# Example new data for prediction\n",
    "new_data = [\n",
    "    {'Make': 'Aston Martin Vanquish', 'Model': '6.0 V12 Coupe 2dr Petrol T-TronicII Euro 5 (565 bhp)', 'Mileage': 33000, 'Year': 2013, 'Prior_Owners': '4', 'Car_Type': 'coupe', 'Sale_Type': 'Private seller'}\n",
    "    # Add more cars as needed\n",
    "]\n",
    "\n",
    "# Convert to DataFrame\n",
    "new_data_df = pd.DataFrame(new_data)\n",
    "\n",
    "# Apply the preprocessing pipeline to the new data\n",
    "X_new_tfidf = vectorizer.transform(new_data_df['Model'])\n",
    "X_new_preprocessed = preprocessor.transform(new_data_df.drop('Model', axis=1))\n",
    "\n",
    "# Convert to dense array if sparse\n",
    "if hasattr(X_new_preprocessed, \"toarray\"):\n",
    "    X_new_preprocessed = X_new_preprocessed.toarray()\n",
    "\n",
    "# Combine TF-IDF features with preprocessed numeric and categorical features\n",
    "X_new_combined = np.hstack((X_new_tfidf.toarray(), X_new_preprocessed))\n",
    "\n",
    "# Make predictions\n",
    "predicted_prices = model.predict(X_new_combined)\n",
    "\n",
    "# Output predicted prices\n",
    "for i, car in enumerate(new_data):\n",
    "    print(f\"Car: {car['Make']}, Predicted Price: Â£{predicted_prices[i]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7cb5bc46-b57d-4e15-9ccc-8f5fb6f9aba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Make                                              Model  \\\n",
      "503      Aston Martin DB7                                    5.9 Vantage 2dr   \n",
      "734      Aston Martin DB7                                            3.2 2dr   \n",
      "636      Aston Martin DB7                                    5.9 Volante 2dr   \n",
      "282      Aston Martin DB9                      6.0 V12 T-TronicII Euro 5 2dr   \n",
      "856      Aston Martin DB9              6.0 V12 Volante T-TronicII Euro 5 2dr   \n",
      "..                    ...                                                ...   \n",
      "536  Aston Martin Vantage  LHD 4.3 V8 Coupe 2dr Petrol Obsidion Black Man...   \n",
      "704  Aston Martin Vantage                                  4.7 V8 Euro 4 2dr   \n",
      "789  Aston Martin Vantage                       4.7 V8 Sportshift Euro 4 2dr   \n",
      "377   Aston Martin Virage                      6.0 V12 T-TronicII Euro 5 2dr   \n",
      "94    Aston Martin Virage              6.0 V12 Volante T-TronicII Euro 5 2dr   \n",
      "\n",
      "           Dealership_Location  Price  Predicted_Price  Price_Diff_Percent  \\\n",
      "503     Dealer locationBeamish  35950     48341.901141           34.469822   \n",
      "734   Dealer locationEdinburgh  39950     48893.220506           22.386034   \n",
      "636                    Unknown  37950     49742.660506           31.074204   \n",
      "282  Dealer locationBarnstable  54995     68890.646332           25.267109   \n",
      "856     Dealer locationBristol  61990     68890.646332           11.131870   \n",
      "..                         ...    ...              ...                 ...   \n",
      "536                    Unknown  55000     61631.924954           12.058045   \n",
      "704                    Unknown  43500     48893.220506           12.398208   \n",
      "789                    Unknown  39995     48893.220506           22.248332   \n",
      "377     Dealer locationBeamish  59950     68890.646332           14.913505   \n",
      "94     Dealer locationCoventry  59990     68890.646332           14.836883   \n",
      "\n",
      "                                             Title_URL          Sale_Type  \n",
      "503  https://www.autotrader.co.uk/car-details/20221...            Unknown  \n",
      "734  https://www.autotrader.co.uk/car-details/20230...            Unknown  \n",
      "636  https://www.autotrader.co.uk/car-details/20220...     Private seller  \n",
      "282  https://www.autotrader.co.uk/car-details/20230...            Unknown  \n",
      "856  https://www.autotrader.co.uk/car-details/20240...            Unknown  \n",
      "..                                                 ...                ...  \n",
      "536  https://www.autotrader.co.uk/car-details/20231...     Private seller  \n",
      "704  https://www.autotrader.co.uk/car-details/20230...  Finance available  \n",
      "789  https://www.autotrader.co.uk/car-details/20230...  Finance available  \n",
      "377  https://www.autotrader.co.uk/car-details/20231...  Finance available  \n",
      "94   https://www.autotrader.co.uk/car-details/20230...            Unknown  \n",
      "\n",
      "[87 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load the saved model, vectorizer, and preprocessor\n",
    "model = joblib.load('car_price_prediction_model_aston_martin.joblib')\n",
    "vectorizer = joblib.load('tfidf_vectorizer_aston_martin.joblib')\n",
    "preprocessor = joblib.load('preprocessor_aston_martin.joblib')\n",
    "\n",
    "# Load the cleaned dataset\n",
    "data = pd.read_excel('cleaned_auto_trader_data_aston_martin.xlsx')\n",
    "\n",
    "# Define preprocessing steps\n",
    "numeric_features = ['Mileage', 'Year', 'Prior_Owners']\n",
    "categorical_features = ['Make', 'Car_Type', 'Sale_Type']\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))]) #! this feels bad we don't want \"missing\" appearing in dataset, unless \"missing\" could actual devalue a car in dealers eyes in reality?\n",
    "\n",
    "# Create a column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Preprocess numeric and categorical features\n",
    "X_preprocessed = preprocessor.fit_transform(data)\n",
    "\n",
    "# Preprocess text feature 'Model'\n",
    "X_text = vectorizer.transform(data['Model'].fillna(''))\n",
    "\n",
    "# Convert to dense array if sparse\n",
    "if hasattr(X_preprocessed, \"toarray\"):\n",
    "    X_preprocessed = X_preprocessed.toarray()\n",
    "if hasattr(X_text, \"toarray\"):\n",
    "    X_text = X_text.toarray()\n",
    "\n",
    "# Combine the processed features\n",
    "X_combined = np.hstack((X_preprocessed, X_text))\n",
    "\n",
    "# Predict prices with the model\n",
    "predicted_prices = model.predict(X_combined)\n",
    "\n",
    "# Add the predicted prices to the DataFrame\n",
    "data['Predicted_Price'] = predicted_prices\n",
    "\n",
    "# Calculate the percentage difference between predicted and actual prices\n",
    "data['Price_Diff_Percent'] = ((data['Predicted_Price'] - data['Price']) / data['Price']) * 100\n",
    "\n",
    "# Filter to find potential undervalued cars with 10-35% price difference # TODO investigate the results in different brackets. Also investigate the distribution of these predictions (are there any outliers)\n",
    "undervalued_cars = data[(data['Price_Diff_Percent'] > 10) & (data['Price_Diff_Percent'] < 35)]\n",
    "\n",
    "# Sort the undervalued cars by Make and Location\n",
    "sorted_undervalued_cars = undervalued_cars.sort_values(['Make', 'Dealership_Location'])\n",
    "\n",
    "# Print out the sorted undervalued cars with 10-35% price difference, including Title_URL\n",
    "print(sorted_undervalued_cars[['Make', 'Model', 'Dealership_Location', 'Price', 'Predicted_Price', 'Price_Diff_Percent', 'Title_URL', 'Sale_Type']])\n",
    "\n",
    "sorted_undervalued_cars.to_csv('sorted_undervalued_cars.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2fdc8f84-55bb-4c99-8f6a-77e783373bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted price of the entered car: 81023.855523803\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Incompatible dimension for X and Y matrices: X.shape[1] == 33 while Y.shape[1] == 110",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 48\u001b[0m\n\u001b[1;32m     45\u001b[0m similar_scores \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m X_target_preprocessed:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# Compute similarity scores\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m     similar_scores\u001b[38;5;241m.\u001b[39mappend(\u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_target_tfidf\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(similar_scores)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Add similarity scores to the dataframe\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/pairwise.py:1657\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m   1613\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[1;32m   1614\u001b[0m \n\u001b[1;32m   1615\u001b[0m \u001b[38;5;124;03mCosine similarity, or the cosine kernel, computes similarity as the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1653\u001b[0m \u001b[38;5;124;03m       [0.57..., 0.81...]])\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m \u001b[38;5;66;03m# to avoid recursive import\u001b[39;00m\n\u001b[0;32m-> 1657\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_pairwise_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1659\u001b[0m X_normalized \u001b[38;5;241m=\u001b[39m normalize(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1660\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m Y:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/pairwise.py:189\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[0;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    184\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecomputed metric requires shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    185\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(n_queries, n_indexed). Got (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    186\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m indexed.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    187\u001b[0m         )\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    190\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible dimension for X and Y matrices: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX.shape[1] == \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m while Y.shape[1] == \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    192\u001b[0m     )\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, Y\n",
      "\u001b[0;31mValueError\u001b[0m: Incompatible dimension for X and Y matrices: X.shape[1] == 33 while Y.shape[1] == 110"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load the cleaned dataset, model, vectorizer, and preprocessor\n",
    "data = pd.read_csv('cleaned_auto_trader_data_aston_martin.csv')\n",
    "model = joblib.load('car_price_prediction_model_aston_martin.joblib')\n",
    "vectorizer = joblib.load('tfidf_vectorizer_aston_martin.joblib')\n",
    "preprocessor = joblib.load('preprocessor_aston_martin.joblib')\n",
    "\n",
    "# Define the target car details\n",
    "target_car = {\n",
    "    'Make': 'Aston Martin', \n",
    "    'Model': 'Vantage 4.3 V8 Sportshift Euro 4 2dr', \n",
    "    'Mileage': 19000, \n",
    "    'Year': 2008, \n",
    "    'Prior_Owners': 1, \n",
    "    'Car_Type': 'Coupe', \n",
    "    'Sale_Type': 'Private seller'\n",
    "}\n",
    "\n",
    "# Convert the target car data into a DataFrame\n",
    "target_car_df = pd.DataFrame([target_car], columns=[key for key in target_car.keys()])\n",
    "\n",
    "# Process target car info for TF-IDF vectorization\n",
    "target_car_df['Combined_Info'] = target_car_df['Make'] + \" \" + target_car_df['Model']\n",
    "X_target_tfidf = vectorizer.transform(target_car_df['Combined_Info'])\n",
    "\n",
    "# Drop the 'Make' and 'Model' columns if they were not used as raw features during training\n",
    "# # Ensure other necessary columns are present\n",
    "# target_car_df = target_car_df.drop(['Make', 'Model', 'Combined_Info'], axis=1)\n",
    "\n",
    "# Transform the rest of the features using the preprocessor\n",
    "X_target_preprocessed = preprocessor.transform(target_car_df)\n",
    "\n",
    "# Combine TF-IDF features with preprocessed numeric and categorical features\n",
    "X_target_combined = np.hstack((X_target_tfidf.toarray(), X_target_preprocessed.toarray()))\n",
    "\n",
    "# Predict the price of the target car\n",
    "predicted_price = model.predict(X_target_combined)\n",
    "print(f\"Predicted price of the entered car: {predicted_price[0]}\")\n",
    "\n",
    "similar_scores = []\n",
    "for data in X_target_preprocessed:\n",
    "    # Compute similarity scores\n",
    "    similar_scores.append(cosine_similarity(data, X_target_tfidf))\n",
    "\n",
    "print(similar_scores)\n",
    "# Add similarity scores to the dataframe\n",
    "data['Similarity'] = similar_scores.flatten()\n",
    "\n",
    "# Sort the dataframe based on similarity\n",
    "similar_cars = data.sort_values(by='Similarity', ascending=False)\n",
    "\n",
    "# Display the top similar cars\n",
    "print(\"Top similar cars:\")\n",
    "print(similar_cars[['Make', 'Model', 'Year', 'Price', 'Mileage', 'Car_Type', 'Sale_Type', 'Similarity']].head())\n",
    "\n",
    "# Save the similar cars to an Excel file\n",
    "similar_cars.to_excel('similar_cars.xlsx', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025a17d8-870f-4405-be6f-93e0be14e1cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
