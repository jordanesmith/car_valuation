{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a21284d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artifacts/filtered/150/PSC\n"
     ]
    }
   ],
   "source": [
    "# SETUP\n",
    "data_file_path = \"data/production/autotrader.csv\"#'data/deprecated/7thMarch/autotrader.csv'#\"data/production/autotrader.csv\"\n",
    "# data_file_path = 'data/legacy/cleaned_auto_trader_data_aston_martin.csv'\n",
    "model_choice = \"xgb\"  # 'rf'\n",
    "filter_max_value = 0.15 * 1e6\n",
    "filter_index_str = str(int(filter_max_value / 1e3))\n",
    "feature_engineer = False\n",
    "retrain = False\n",
    "fe_str = \"fe\" if feature_engineer else \"\"\n",
    "car_make = \"PSC\"\n",
    "model_path = f\"artifacts/filtered/{filter_index_str}/{fe_str}{car_make}\"\n",
    "\n",
    "car_make_mapped = {\n",
    "    \"AM\": \"Aston Martin\",\n",
    "    \"JAG\": \"Jaguar\",\n",
    "    \"BMW\": \"BMW\",\n",
    "    \"MB\": \"Mercedes-Benz\",\n",
    "    \"LT\": \"Lotus\",\n",
    "    \"AR\": \"Alfa Romeo\",\n",
    "    \"LR\": \"Land Rover\",\n",
    "    \"BTL\": \"Bentley\",\n",
    "    \"PSC\": \"Porsche\",\n",
    "}\n",
    "\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a26d0696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Preprocessing numeric columns...\n",
      "BEFORE price FILTER: 2267\n",
      "AFTER price FILTER: 2219\n",
      "BEFORE make FILTER: 2142\n",
      "AFTER make FILTER: 507\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# PREPROCESS\n",
    "\n",
    "\n",
    "\n",
    "# Load the data\n",
    "print(\"Loading data...\")\n",
    "data = pd.read_csv(data_file_path)\n",
    "\n",
    "# Preprocess numeric columns\n",
    "print(\"Preprocessing numeric columns...\")\n",
    "for col in ['Price', 'Mileage', 'Year', 'Prior_Owners']:\n",
    "    data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "    # data[col].fillna(data[col].mean(), inplace=True)\n",
    "\n",
    "# !Filter the data \n",
    "# by price\n",
    "print(\"BEFORE price FILTER:\",len(data))\n",
    "data = data[data['Price'] <= filter_max_value] #! filtering\n",
    "print(\"AFTER price FILTER:\",len(data))\n",
    "\n",
    "# by model of car\n",
    "data.dropna(subset=['Make'], inplace=True)\n",
    "print(\"BEFORE make FILTER:\",len(data))\n",
    "data = data[data['Make'].str.contains(car_make_mapped[car_make], case=False)]\n",
    "print(\"AFTER make FILTER:\",len(data))\n",
    "\n",
    "# Create the pair plot\n",
    "numeric_features_to_plot = ['Mileage', 'Year', 'Prior_Owners', 'Price']\n",
    "\n",
    "# Function to extract numeric part from a string (for 'Price' and 'Mileage')\n",
    "def extract_numeric(value):\n",
    "    try:\n",
    "        # Remove non-digit characters, preserve decimal points and negative signs\n",
    "        numeric_string = re.sub(\"[^-0-9.]\", \"\", str(value))\n",
    "        return float(numeric_string)\n",
    "    except ValueError:\n",
    "        return np.nan  # Return NaN for non-numeric or empty values\n",
    "    \n",
    "    \n",
    "\n",
    "# Clean the 'Year' column by removing parentheses and their contents\n",
    "data['Year'] = data['Year'].astype(str).apply(lambda x: re.sub(r'\\s*\\(.*\\)', '', x)).astype(float)\n",
    "\n",
    "# Apply transformations and validations\n",
    "data['Price'] = data['Price'].apply(extract_numeric)\n",
    "data['Mileage'] = data['Mileage'].apply(extract_numeric)\n",
    "data['Prior_Owners'] = data['Prior_Owners'].apply(extract_numeric)  # Updated to use extract_numeric\n",
    "data['Engine_Size'] = data['Engine_Size'].apply(extract_numeric)\n",
    "\n",
    "data = data[data['Engine_Size'] <= 10] # remove anomaly\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Fill in missing data with 'Unknown' for other categorical columns\n",
    "categorical_columns = ['Model', 'Additional_Comments', 'Car_Type', 'Engine_Type', 'Dealership_Location', 'Sale_Type']\n",
    "data[categorical_columns] = data[categorical_columns].fillna('Unknown')\n",
    "\n",
    "# Remove rows with 'Unknown' values in critical columns\n",
    "data = data[data['Price'] != 'Unknown']\n",
    "data = data[data['Mileage'] != 'Unknown']\n",
    "data = data[data['Year'] != 'Unknown']\n",
    "data = data[data['Engine_Size'] != 'Unknown']\n",
    "\n",
    "\n",
    "# Adding the delete duplicates feature\n",
    "cleaned_data = data.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f8980cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EXPLORATORY DATA ANALYSIS\n",
    "\n",
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Assuming your dataset is loaded into a DataFrame named df\n",
    "# # df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# # Example: Visualizing the relationship between Mileage and Price for different Car Types\n",
    "# sns.scatterplot(data=data, x='Mileage', y='Price', hue='Car_Type')\n",
    "# plt.title('Price vs. Mileage by Car Type')\n",
    "# plt.show()\n",
    "\n",
    "# # Assuming 'df' is your DataFrame with 'car_type' and 'price' columns\n",
    "\n",
    "\n",
    "# # Boxplot to visualize the distribution of prices for each car type\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# sns.boxplot(x='Car_Type', y='Price', data=data)\n",
    "# plt.title('Price Distribution by Car Type')\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.show()\n",
    "\n",
    "# # Plotting Make and Year vs Price\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.scatterplot(x='Year', y='Price', hue='Make', data=data)\n",
    "# plt.title('Price by Year and Make')\n",
    "# plt.show()\n",
    "\n",
    "# # Plotting Mileage and Year vs Price\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.scatterplot(x='Year', y='Price', hue='Mileage', palette=\"coolwarm\", data=data)\n",
    "# plt.title('Price by Year and Mileage')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7ef9bd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE ENGINEERING\n",
    "\n",
    "if feature_engineer:\n",
    "    # Interaction terms\n",
    "    data['Make_Year'] = data['Make'] + '_' + data['Year'].astype(str)\n",
    "    data['Mileage_Year'] = data['Mileage'] * data['Year']\n",
    "    data['EngineSize_CarType'] = data['Engine_Size'].astype(str) + '_' + data['Car_Type']\n",
    "    data['Transmission_EngineType'] = data['Transmission'] + '_' + data['Engine_Type']\n",
    "\n",
    "    # Encode categorical variables if you haven't already\n",
    "    data = pd.get_dummies(data, drop_first=True, columns=['Make_Year', 'Mileage_Year', 'EngineSize_CarType', 'Transmission_EngineType'])\n",
    "\n",
    "    data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9212a467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing numeric columns...\n",
      "Creating preprocessing pipelines...\n",
      "Applying TF-IDF Vectorization...\n",
      "Splitting data...\n",
      "DONE\n",
      "DATASET SIZE: 493\n",
      "(493, 85)\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.29440618 0.25031541 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.55529059 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.25563703\n",
      " 0.         0.52502704 0.         0.         0.         0.\n",
      " 0.33991812 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.29286779 0.                nan 1.76092872 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         1.         0.         0.\n",
      " 0.         0.         0.         1.         1.         0.\n",
      " 0.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1f/kh62j3491pg82ts5mywnf4hc0000gn/T/ipykernel_35221/191330539.py:20: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[col].fillna(data[col].mean(), inplace=True)\n",
      "/Users/jordan/code/vroom_swish/vroom/lib/python3.9/site-packages/sklearn/utils/extmath.py:1137: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "/Users/jordan/code/vroom_swish/vroom/lib/python3.9/site-packages/sklearn/utils/extmath.py:1142: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "/Users/jordan/code/vroom_swish/vroom/lib/python3.9/site-packages/sklearn/utils/extmath.py:1162: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    }
   ],
   "source": [
    "# PREPROCESS\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "\n",
    "\n",
    "# Preprocess numeric columns\n",
    "print(\"Preprocessing numeric columns...\")\n",
    "for col in ['Price', 'Mileage', 'Year', 'Prior_Owners']:\n",
    "    data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "    data[col].fillna(data[col].mean(), inplace=True)\n",
    "\n",
    "# Preprocessing pipelines\n",
    "print(\"Creating preprocessing pipelines...\") # TODO visualise the data after and before preprocessing to see if any issues with it\n",
    "numeric_features = ['Mileage', 'Year', 'Prior_Owners']\n",
    "numeric_transformer = StandardScaler()\n",
    "\n",
    "categorical_features = ['Make', 'Car_Type', 'Sale_Type']\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# TF-IDF Vectorizer for 'Model' column\n",
    "print(\"Applying TF-IDF Vectorization...\") # TODO check results of this vectorisation. not sure TF-IDF is needed for \"Model\" of car tbh but interesting.\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=2500)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(data['Model'])\n",
    "\n",
    "# Fit and transform the data using the preprocessor\n",
    "X_preprocessed = preprocessor.fit_transform(data.drop('Model', axis=1)) # TODO check the data here is not a concatination, but just the preprocessed data \n",
    "\n",
    "# Combine TF-IDF features with preprocessed numeric and categorical features # TODO investigate how this data looks, both TFIDF and preprocessed, to see if we really can expect model to fit to both at same time\n",
    "X_combined = np.hstack((X_tfidf.toarray(), X_preprocessed if isinstance(X_preprocessed, np.ndarray) else X_preprocessed.toarray()))\n",
    "# TODO I think bunching this data together and using random forest isn't a good combination. Need to visualise both the TFIDF data and preprocessed data, seperately, and both before and after preprocessing. Do some exploratory data analysis. Visualise words with word map (bigger words for more frequent) and visualise numeric data with grid map different variables on each axis of grid\n",
    "\n",
    "# Split the data\n",
    "print(\"Splitting data...\")\n",
    "y = data['Price'].values\n",
    "\n",
    "if retrain: \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "else:\n",
    "    X_test = X_combined\n",
    "    \n",
    "print(\"DONE\")\n",
    "print(\"DATASET SIZE:\",len(y))\n",
    "print(X_test.shape)\n",
    "print(X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f4bb79f-4cf5-407d-9d6d-2b4231f3c925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if retrain:\n",
    "#     # Model Training with Hyperparameter Tuning\n",
    "#     print(\"Training model with hyperparameter tuning...\")\n",
    "#     rf_model = RandomForestRegressor(random_state=42)\n",
    "#     param_grid = {\n",
    "#         'n_estimators': [20, 50, 100, 200],\n",
    "#         'max_depth': [None, 10, 20, 30],\n",
    "#         'min_samples_split': [2, 5, 10]\n",
    "#     }\n",
    "#     grid_search = GridSearchCV(rf_model, param_grid, cv=5)\n",
    "#     grid_search.fit(X_train, y_train)\n",
    "\n",
    "#     # Use the best estimator # TODO investigate the \"performance\" of each model with different hyperparamter tuning. Check it's actually finding a good one, not just the least poor performing.\n",
    "#     best_rf_model = grid_search.best_estimator_\n",
    "#     print(\"Best model found:\", best_rf_model)\n",
    "\n",
    "#     # Predictions and Evaluation\n",
    "#     print(\"Making predictions and evaluating...\")\n",
    "#     y_pred = best_rf_model.predict(X_test)\n",
    "#     mse = mean_squared_error(y_test, y_pred)\n",
    "#     mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "#     r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "#     # Output the model performance\n",
    "#     print(f'MSE: {mse}')\n",
    "#     print(f'MAE: {mae}')  \n",
    "#     print(f'R2 Score: {r2}') # TODO this score seems good. but not great. Investigate examples where it's accurate, and where it's messing up. see if there's anything obvious. back to EDA\n",
    "\n",
    "\n",
    "#     # Save the model, vectorizer, and preprocessor\n",
    "#     print(\"Saving model, vectorizer, and preprocessor...\")\n",
    "#     joblib.dump(best_rf_model, f'{model_path}random_forest_car_price_predictor.joblib')\n",
    "#     joblib.dump(tfidf_vectorizer, f'{model_path}tfidf_vectorizer.joblib')\n",
    "#     joblib.dump(preprocessor, f'{model_path}preprocessor.joblib')\n",
    "\n",
    "#     print(\"Process completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9183aec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import joblib\n",
    "\n",
    "if retrain:\n",
    "    print(f\"Training XGBoost model for {car_make} cars\")\n",
    "\n",
    "    # Switch to XGBRegressor\n",
    "    xgb_model = XGBRegressor(random_state=42)\n",
    "\n",
    "    # Adjusting the param_grid for XGBoost-specific hyperparameters\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [6, 10, 15],  # XGBoost often uses smaller depths\n",
    "        'learning_rate': [0.01, 0.1, 0.2],  # Also known as eta\n",
    "        'subsample': [0.8, 1],  # Subsample ratio of the training instances\n",
    "        'colsample_bytree': [0.8, 1],  # Subsample ratio of columns when constructing each tree\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(xgb_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Use the best estimator\n",
    "    best_xgb_model = grid_search.best_estimator_\n",
    "    print(\"Best model found:\", best_xgb_model)\n",
    "\n",
    "    # Predictions and Evaluation\n",
    "    print(\"Making predictions and evaluating...\")\n",
    "    y_pred = best_xgb_model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    dataset_size = len(X_train)\n",
    "\n",
    "    mse_formatted = f'{mse:.4g}'\n",
    "    mae_formatted = f'{mae:.4g}'\n",
    "    r2_formatted = f'{r2:.4g}'\n",
    "    dataset_size_formatted = f'{dataset_size:.4g}'    \n",
    "    \n",
    "\n",
    "    # Output the model performance\n",
    "    print(f'MSE: {mse}')\n",
    "    print(f'MAE: {mae}')\n",
    "    print(f'R2 Score: {r2}')\n",
    "    \n",
    "    with open('training_results.csv', 'a') as file:\n",
    "        file.write(f\"{car_make}, {model_choice},{mse},{mae},{r2},{dataset_size}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    # Save the model, and potentially other artifacts if they are part of your workflow\n",
    "    print(\"Saving model...\")\n",
    "    joblib.dump(best_xgb_model, f'{model_path}xgboost_car_price_predictor.joblib')\n",
    "\n",
    "    print(\"Process completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2a8ff9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded successfully.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Feature shape mismatch, expected: 101, got 85",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Make predictions with the loaded model\u001b[39;00m\n\u001b[1;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m models[model_choice]\n\u001b[0;32m---> 12\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m mse \u001b[38;5;241m=\u001b[39m mean_squared_error(y_test, y_pred)\n\u001b[1;32m     14\u001b[0m mae \u001b[38;5;241m=\u001b[39m mean_absolute_error(y_test, y_pred)\n",
      "File \u001b[0;32m~/code/vroom_swish/vroom/lib/python3.9/site-packages/xgboost/sklearn.py:1168\u001b[0m, in \u001b[0;36mXGBModel.predict\u001b[0;34m(self, X, output_margin, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_can_use_inplace_predict():\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1168\u001b[0m         predts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_booster\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace_predict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m            \u001b[49m\u001b[43miteration_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miteration_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmargin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput_margin\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1176\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _is_cupy_array(predts):\n\u001b[1;32m   1177\u001b[0m             \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcupy\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=import-error\u001b[39;00m\n",
      "File \u001b[0;32m~/code/vroom_swish/vroom/lib/python3.9/site-packages/xgboost/core.py:2428\u001b[0m, in \u001b[0;36mBooster.inplace_predict\u001b[0;34m(self, data, iteration_range, predict_type, missing, validate_features, base_margin, strict_shape)\u001b[0m\n\u001b[1;32m   2424\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   2425\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`shape` attribute is required when `validate_features` is True.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2426\u001b[0m         )\n\u001b[1;32m   2427\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_features() \u001b[38;5;241m!=\u001b[39m data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m-> 2428\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2429\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature shape mismatch, expected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_features()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2430\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2431\u001b[0m         )\n\u001b[1;32m   2433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_np_array_like(data):\n\u001b[1;32m   2434\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _ensure_np_dtype\n",
      "\u001b[0;31mValueError\u001b[0m: Feature shape mismatch, expected: 101, got 85"
     ]
    }
   ],
   "source": [
    "# LOAD MODELS\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Load the saved model\n",
    "xgb_model = joblib.load(f'{model_path}xgboost_car_price_predictor.joblib')\n",
    "rf_model = None #joblib.load(f'{model_path}random_forest_car_price_predictor.joblib')\n",
    "models = {'xgb':xgb_model, 'rf': rf_model}\n",
    "print(\"Models loaded successfully.\")\n",
    "\n",
    "\n",
    "# Make predictions with the loaded model\n",
    "model = models[model_choice]\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"Predictions made on new data:\", y_pred)\n",
    "\n",
    "# Now you can use y_pred_new as needed for your application\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb399d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price Range 0-50000:\n",
      "  MSE: 18696617.885367457\n",
      "  MAE: 2988.475310724432\n",
      "Price Range 50000-100000:\n",
      "  MSE: 192835543.2094326\n",
      "  MAE: 9800.58740234375\n",
      "Price Range 100000-150000: No data available\n"
     ]
    }
   ],
   "source": [
    "# DIVING INTO INFERENCE ACCURACY\n",
    "\n",
    "# Analyze performance in different price categories\n",
    "# price_categories = [(0, 100000), (100000, 200000), (200000, 300000), \n",
    "#                     (300000, 400000), (400000, 500000), (500000, 600000), \n",
    "#                     (600000, 700000), (700000, 800000), (800000, 900000), \n",
    "#                     (900000, 1000000), (1000000, np.inf)]\n",
    "price_categories = [(0, 50000), (50000, 100000), (100000, 150000)]\n",
    "\n",
    "\n",
    "for lower, upper in price_categories:\n",
    "    mask = (y_test >= lower) & (y_test < upper)\n",
    "    y_test_segment = y_test[mask]\n",
    "    y_pred_segment = y_pred[mask]\n",
    "\n",
    "    if len(y_test_segment) > 0:\n",
    "        mse_segment = mean_squared_error(y_test_segment, y_pred_segment)\n",
    "        mae_segment = mean_absolute_error(y_test_segment, y_pred_segment)\n",
    "\n",
    "        print(f\"Price Range {lower}-{upper if upper != np.inf else '1M+'}:\")\n",
    "        print(f\"  MSE: {mse_segment}\")\n",
    "        print(f\"  MAE: {mae_segment}\")\n",
    "    else:\n",
    "        print(f\"Price Range {lower}-{upper if upper != np.inf else '1M+'}: No data available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1347b0eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predicted, residuals, line_x, line_y, std_residuals\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Calculate for both original and filtered datasets\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m predicted_original, residuals_original, line_x_orig, line_y_orig, std_residuals_orig \u001b[38;5;241m=\u001b[39m calculate_residuals_and_stats(y_test, \u001b[43my_pred\u001b[49m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# predicted_filtered, residuals_filtered, line_x_filt, line_y_filt, std_residuals_filt = calculate_residuals_and_stats(y_test_filtered, y_pred_filtered)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Plotting\u001b[39;00m\n\u001b[1;32m     31\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "#PLOT RESIDUALS\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Assuming y_test, y_pred, y_test_filtered, y_pred_filtered are defined\n",
    "\n",
    "def calculate_residuals_and_stats(y_test, y_pred):\n",
    "    \"\"\"Calculate residuals, predicted values, and stats for given test and prediction arrays.\"\"\"\n",
    "    residuals = y_test - y_pred\n",
    "    predicted = y_pred\n",
    "\n",
    "    # Calculate and print statistics\n",
    "    std_residuals = np.std(residuals)\n",
    "    mean_residuals = np.mean(residuals)\n",
    "    print(f\"STD Residuals: {std_residuals}\")\n",
    "    print(f\"MEAN Residuals: {mean_residuals}\")\n",
    "\n",
    "    # Calculate coefficients for the line of best fit\n",
    "    slope, intercept = np.polyfit(predicted, residuals, 1)\n",
    "    print(\"SLOPE\", slope)\n",
    "    line_x = np.linspace(min(predicted), max(predicted), 100)\n",
    "    line_y = slope * line_x + intercept\n",
    "\n",
    "    return predicted, residuals, line_x, line_y, std_residuals\n",
    "\n",
    "# Calculate for both original and filtered datasets\n",
    "predicted_original, residuals_original, line_x_orig, line_y_orig, std_residuals_orig = calculate_residuals_and_stats(y_test, y_pred)\n",
    "# predicted_filtered, residuals_filtered, line_x_filt, line_y_filt, std_residuals_filt = calculate_residuals_and_stats(y_test_filtered, y_pred_filtered)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Original dataset residuals\n",
    "plt.scatter(predicted_original, residuals_original, alpha=0.5, color='blue', label='Original')\n",
    "plt.plot(line_x_orig, line_y_orig, 'b--', linewidth=2, label='Best Fit Original')\n",
    "plt.fill_between(line_x_orig, line_y_orig - std_residuals_orig, line_y_orig + std_residuals_orig, color='blue', alpha=0.1)\n",
    "\n",
    "# Filtered dataset residuals\n",
    "# plt.scatter(predicted_filtered, residuals_filtered, alpha=0.5, color='green', label='Filtered')\n",
    "# plt.plot(line_x_filt, line_y_filt, 'g--', linewidth=2, label='Best Fit Filtered')\n",
    "# plt.fill_between(line_x_filt, line_y_filt - std_residuals_filt, line_y_filt + std_residuals_filt, color='green', alpha=0.1)\n",
    "\n",
    "plt.title(f'Comparison of Residuals: {model_choice}')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.xlim(20000,150000)\n",
    "plt.ylim(-50000,50000)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel(\"actual price\")\n",
    "plt.ylabel(\"predicted price\")\n",
    "plt.title(f\"{model_choice} {fe_str}\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88788a4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m mask \u001b[38;5;241m=\u001b[39m (y_test \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m lower) \u001b[38;5;241m&\u001b[39m (y_test \u001b[38;5;241m<\u001b[39m upper)\n\u001b[1;32m     20\u001b[0m y_test_segment \u001b[38;5;241m=\u001b[39m y_test[mask]\n\u001b[0;32m---> 21\u001b[0m y_pred_segment \u001b[38;5;241m=\u001b[39m \u001b[43my_pred\u001b[49m[mask]\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_test_segment) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# Calculate MSE and MAE for this segment\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     mse_values\u001b[38;5;241m.\u001b[39mappend(mean_squared_error(y_test_segment, y_pred_segment))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "# CATEGORISE BY PRICE RANGE\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have y_test and y_pred as your actual and predicted values\n",
    "\n",
    "# Define your price ranges\n",
    "# price_categories = [(0, 100000), (100000, 200000), (200000, 300000), \n",
    "#                     (300000, 400000), (500000, 600000), (600000, 700000)]\n",
    "price_categories = [(0, 50000), (50000, 100000), (100000, 150000)]\n",
    "\n",
    "# Initialize lists to store metrics and evidence count\n",
    "mse_values = []\n",
    "mae_values = []\n",
    "mae_proportional_values = []\n",
    "evidence_counts = []\n",
    "price_range_labels = []\n",
    "\n",
    "for lower, upper in price_categories:\n",
    "    mask = (y_test >= lower) & (y_test < upper)\n",
    "    y_test_segment = y_test[mask]\n",
    "    y_pred_segment = y_pred[mask]\n",
    "\n",
    "    if len(y_test_segment) > 0:\n",
    "        # Calculate MSE and MAE for this segment\n",
    "        mse_values.append(mean_squared_error(y_test_segment, y_pred_segment))\n",
    "        # mae_values.append(mean_absolute_error(y_test_segment, y_pred_segment))\n",
    "        mae_value = mean_absolute_error(y_test_segment, y_pred_segment)\n",
    "        mean_price = np.mean(y_test_segment)\n",
    "        mae_proportional = mae_value / mean_price  # Proportional MAE\n",
    "        mae_proportional_values.append(mae_proportional)\n",
    "        evidence_counts.append(len(y_test_segment))\n",
    "        price_range_labels.append(f\"{lower/1000}K-{upper/1000}K\")\n",
    "\n",
    "# Create subplots\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plotting MSE\n",
    "ax1.bar(price_range_labels, mse_values, color='b', alpha=0.6, label='MSE')\n",
    "ax1.set_xlabel('Price Range')\n",
    "ax1.set_ylabel('MSE', color='b')\n",
    "ax1.tick_params('y', colors='b')\n",
    "\n",
    "# Creating a second y-axis for MAE\n",
    "ax2 = ax1.twinx()\n",
    "# ax2.plot(price_range_labels, mae_values, color='r', marker='o', label='MAE')\n",
    "ax2.plot(price_range_labels, mae_proportional_values, color='r', marker='o', label='Proportional MAE')\n",
    "\n",
    "# ax2.set_ylabel('MAE', color='r')\n",
    "ax2.set_ylabel('Proportional MAE', color='r')\n",
    "\n",
    "ax2.tick_params('y', colors='r')\n",
    "\n",
    "# Creating a third y-axis for Evidence Count\n",
    "ax3 = ax1.twinx()\n",
    "ax3.spines['right'].set_position(('outward', 60))  # Offset the right spine of ax3\n",
    "ax3.plot(price_range_labels, evidence_counts, color='g', marker='x', label='Data Points')\n",
    "ax3.set_ylabel('Number of Data Points', color='g')\n",
    "ax3.tick_params('y', colors='g')\n",
    "\n",
    "# Title and legends\n",
    "plt.title('MSE, MAE, and Data Points for Different Price Ranges')\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='lower left')\n",
    "ax3.legend(loc='upper right')\n",
    "print(\"MSE\", np.round(np.array(mse_values)/1e8,2))\n",
    "print(\"pMAE\", np.round(np.array(mae_proportional_values),3))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9b6d1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = np.vstack((X_train, X_test))\n",
    "Y_ = np.hstack((y_train, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4562b12e",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'output'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[126], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m most_overvalued_filtered \u001b[38;5;241m=\u001b[39m filtered_data\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrice_Difference\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Save the results to CSV files\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[43mmost_undervalued_filtered\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcar_make\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43mmost_undervalued_cars.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m most_overvalued_filtered\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcar_make\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mmost_overvalued_cars.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/code/vroom_swish/vroom/lib/python3.9/site-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/vroom_swish/vroom/lib/python3.9/site-packages/pandas/core/generic.py:3964\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3953\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[1;32m   3955\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[1;32m   3956\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[1;32m   3957\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3961\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[1;32m   3962\u001b[0m )\n\u001b[0;32m-> 3964\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3967\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3969\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3972\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3981\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/vroom_swish/vroom/lib/python3.9/site-packages/pandas/io/formats/format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[1;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[1;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[1;32m   1013\u001b[0m )\n\u001b[0;32m-> 1014\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[0;32m~/code/vroom_swish/vroom/lib/python3.9/site-packages/pandas/io/formats/csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[1;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[1;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[1;32m    268\u001b[0m     )\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[0;32m~/code/vroom_swish/vroom/lib/python3.9/site-packages/pandas/io/common.py:749\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[0;32m--> 749\u001b[0m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[0;32m~/code/vroom_swish/vroom/lib/python3.9/site-packages/pandas/io/common.py:616\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    614\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[0;32m--> 616\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'output'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Assuming df is your DataFrame and contains a 'Price' column for the actual prices\n",
    "# And assuming your features for prediction are ready in df\n",
    "\n",
    "# Predict prices using the trained model\n",
    "y_pred = model.predict(X_)\n",
    "\n",
    "\n",
    "# Calculate the difference between predicted and actual prices\n",
    "data['Price_Difference'] = Y_ - y_pred\n",
    "filtered_data = data[abs(data['Price_Difference']) <= 20000]\n",
    "\n",
    "# Find the 10 most undervalued cars in the filtered dataset\n",
    "most_undervalued_filtered = filtered_data.sort_values(by='Price_Difference', ascending=True).head(10)\n",
    "\n",
    "# Find the 10 most overvalued cars in the filtered dataset\n",
    "most_overvalued_filtered = filtered_data.sort_values(by='Price_Difference', ascending=False).head(10)\n",
    "\n",
    "# Save the results to CSV files\n",
    "most_undervalued_filtered.to_csv(f'output/{car_make}most_undervalued_cars.csv', index=False)\n",
    "most_overvalued_filtered.to_csv(f'output/{car_make}most_overvalued_cars.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e0fd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | max_depth | n_esti... |\n",
      "-------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m-1.294e+0\u001b[0m | \u001b[0m14.26    \u001b[0m | \u001b[0m182.9    \u001b[0m |\n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m-2.301e+0\u001b[0m | \u001b[0m3.003    \u001b[0m | \u001b[0m82.56    \u001b[0m |\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m-1.487e+0\u001b[0m | \u001b[0m6.962    \u001b[0m | \u001b[0m32.16    \u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m-1.307e+0\u001b[0m | \u001b[0m8.029    \u001b[0m | \u001b[0m92.93    \u001b[0m |\n",
      "| \u001b[95m5        \u001b[0m | \u001b[95m-1.236e+0\u001b[0m | \u001b[95m13.71    \u001b[0m | \u001b[95m139.3    \u001b[0m |\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m-1.296e+0\u001b[0m | \u001b[0m14.32    \u001b[0m | \u001b[0m174.5    \u001b[0m |\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m-1.298e+0\u001b[0m | \u001b[0m8.52     \u001b[0m | \u001b[0m220.7    \u001b[0m |\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m-2.259e+0\u001b[0m | \u001b[0m3.739    \u001b[0m | \u001b[0m170.9    \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m-1.279e+0\u001b[0m | \u001b[0m14.27    \u001b[0m | \u001b[0m144.1    \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m-1.462e+0\u001b[0m | \u001b[0m6.79     \u001b[0m | \u001b[0m57.54    \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m-1.25e+08\u001b[0m | \u001b[0m20.51    \u001b[0m | \u001b[0m178.3    \u001b[0m |\n",
      "| \u001b[95m12       \u001b[0m | \u001b[95m-1.14e+08\u001b[0m | \u001b[95m12.28    \u001b[0m | \u001b[95m100.8    \u001b[0m |\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m-2.296e+0\u001b[0m | \u001b[0m3.226    \u001b[0m | \u001b[0m103.8    \u001b[0m |\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m-1.217e+0\u001b[0m | \u001b[0m14.85    \u001b[0m | \u001b[0m95.9     \u001b[0m |\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m-1.198e+0\u001b[0m | \u001b[0m18.84    \u001b[0m | \u001b[0m103.2    \u001b[0m |\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m-1.935e+0\u001b[0m | \u001b[0m4.503    \u001b[0m | \u001b[0m141.1    \u001b[0m |\n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m-1.237e+0\u001b[0m | \u001b[0m21.93    \u001b[0m | \u001b[0m140.3    \u001b[0m |\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/code/vroom_swish/vroom/lib/python3.9/site-packages/bayes_opt/bayesian_optimization.py:305\u001b[0m, in \u001b[0;36mBayesianOptimization.maximize\u001b[0;34m(self, init_points, n_iter, acquisition_function, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 305\u001b[0m     x_probe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_queue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m~/code/vroom_swish/vroom/lib/python3.9/site-packages/bayes_opt/bayesian_optimization.py:27\u001b[0m, in \u001b[0;36mQueue.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueue is empty, no more objects to retrieve.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_queue[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mStopIteration\u001b[0m: Queue is empty, no more objects to retrieve.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 31\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Bayesian optimization\u001b[39;00m\n\u001b[1;32m     25\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m BayesianOptimization(\n\u001b[1;32m     26\u001b[0m     f\u001b[38;5;241m=\u001b[39mrfr_model,\n\u001b[1;32m     27\u001b[0m     pbounds\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m250\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m30\u001b[39m)},\n\u001b[1;32m     28\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     29\u001b[0m )\n\u001b[0;32m---> 31\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Output the optimal parameters\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(optimizer\u001b[38;5;241m.\u001b[39mmax)\n",
      "File \u001b[0;32m~/code/vroom_swish/vroom/lib/python3.9/site-packages/bayes_opt/bayesian_optimization.py:308\u001b[0m, in \u001b[0;36mBayesianOptimization.maximize\u001b[0;34m(self, init_points, n_iter, acquisition_function, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    307\u001b[0m     util\u001b[38;5;241m.\u001b[39mupdate_params()\n\u001b[0;32m--> 308\u001b[0m     x_probe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuggest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mutil\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m     iteration \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobe(x_probe, lazy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/code/vroom_swish/vroom/lib/python3.9/site-packages/bayes_opt/bayesian_optimization.py:226\u001b[0m, in \u001b[0;36mBayesianOptimization.suggest\u001b[0;34m(self, utility_function)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstraint\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_space\u001b[38;5;241m.\u001b[39mparams,\n\u001b[1;32m    223\u001b[0m                             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_space\u001b[38;5;241m.\u001b[39m_constraint_values)\n\u001b[1;32m    225\u001b[0m \u001b[38;5;66;03m# Finding argmax of the acquisition function.\u001b[39;00m\n\u001b[0;32m--> 226\u001b[0m suggestion \u001b[38;5;241m=\u001b[39m \u001b[43macq_max\u001b[49m\u001b[43m(\u001b[49m\u001b[43mac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mutility_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutility\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mgp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mconstraint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstraint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m                     \u001b[49m\u001b[43my_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_random_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_space\u001b[38;5;241m.\u001b[39marray_to_params(suggestion)\n",
      "File \u001b[0;32m~/code/vroom_swish/vroom/lib/python3.9/site-packages/bayes_opt/util.py:78\u001b[0m, in \u001b[0;36macq_max\u001b[0;34m(ac, gp, y_max, bounds, random_state, constraint, n_warmup, n_iter)\u001b[0m\n\u001b[1;32m     74\u001b[0m     to_minimize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;241m-\u001b[39mac(x\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), gp\u001b[38;5;241m=\u001b[39mgp, y_max\u001b[38;5;241m=\u001b[39my_max)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_try \u001b[38;5;129;01min\u001b[39;00m x_seeds:\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;66;03m# Find the minimum of minus the acquisition function\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_minimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mx_try\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mL-BFGS-B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;66;03m# See if success\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m res\u001b[38;5;241m.\u001b[39msuccess:\n",
      "File \u001b[0;32m~/code/vroom_swish/vroom/lib/python3.9/site-packages/scipy/optimize/_minimize.py:713\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    710\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    711\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 713\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    716\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[1;32m    717\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[0;32m~/code/vroom_swish/vroom/lib/python3.9/site-packages/scipy/optimize/_lbfgsb_py.py:369\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    363\u001b[0m task_str \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFG\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[1;32m    368\u001b[0m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[0;32m--> 369\u001b[0m     f, g \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEW_X\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[1;32m    372\u001b[0m     n_iterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/code/vroom_swish/vroom/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py:295\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfun_and_grad\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray_equal(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx):\n\u001b[0;32m--> 295\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_x_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun()\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_grad()\n",
      "File \u001b[0;32m~/code/vroom_swish/vroom/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py:253\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_x\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_x\u001b[39m(x):\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;66;03m# ensure that self.x is a copy of x. Don't store a reference\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;66;03m# otherwise the memoization doesn't work properly.\u001b[39;00m\n\u001b[0;32m--> 253\u001b[0m     _x \u001b[38;5;241m=\u001b[39m \u001b[43matleast_nd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxp\u001b[38;5;241m.\u001b[39mastype(_x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_dtype)\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/code/vroom_swish/vroom/lib/python3.9/site-packages/scipy/_lib/_array_api.py:161\u001b[0m, in \u001b[0;36matleast_nd\u001b[0;34m(x, ndim, xp)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    160\u001b[0m     xp \u001b[38;5;241m=\u001b[39m array_namespace(x)\n\u001b[0;32m--> 161\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m<\u001b[39m ndim:\n\u001b[1;32m    163\u001b[0m     x \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mexpand_dims(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/code/vroom_swish/vroom/lib/python3.9/site-packages/scipy/_lib/array_api_compat/array_api_compat/common/_aliases.py:300\u001b[0m, in \u001b[0;36m_asarray\u001b[0;34m(obj, dtype, device, copy, namespace, **kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m         \u001b[38;5;66;03m# TODO: What about lists of arrays?\u001b[39;00m\n\u001b[1;32m    299\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA namespace must be specified for asarray() with non-array input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 300\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mModuleType\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    301\u001b[0m     xp \u001b[38;5;241m=\u001b[39m namespace\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m namespace \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# BAYESIAN OPTIMISATION OF HYPERPARAMETERS\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# Split the data\n",
    "df = pd.get_dummies(data, drop_first=True)\n",
    "\n",
    "X = X_combined\n",
    "y = df['Price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model and optimization function\n",
    "def rfr_model(n_estimators, max_depth):\n",
    "    # Model definition\n",
    "    model = RandomForestRegressor(n_estimators=int(n_estimators), \n",
    "                                  max_depth=int(max_depth), \n",
    "                                  random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, preds)\n",
    "    return -mse\n",
    "\n",
    "# Bayesian optimization\n",
    "optimizer = BayesianOptimization(\n",
    "    f=rfr_model,\n",
    "    pbounds={'n_estimators': (10, 250), 'max_depth': (3, 30)},\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "optimizer.maximize(init_points=10, n_iter=25)\n",
    "\n",
    "# Output the optimal parameters\n",
    "print(optimizer.max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025a17d8-870f-4405-be6f-93e0be14e1cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698ba1d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6de6c06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefef1a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
